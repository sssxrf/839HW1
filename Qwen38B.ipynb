{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71abcded",
   "metadata": {},
   "source": [
    "### Model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6669bd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\UWMadisonCourses\\2025Fall\\CS839\\HW1\\839HW1\\myEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 10.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF total: 8,190,735,360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "n = sum(p.numel() for p in model.parameters())\n",
    "print(f\"HF total: {n:,}\")  # ~8,190,722,048 (8.19B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830e273",
   "metadata": {},
   "source": [
    "### Component wise calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af1836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hardcoded (formula) per-layer counts ===\n",
      "attn : 41,943,040\n",
      "mlp  : 150,994,944\n",
      "rms  : 8,192\n",
      "qkn  : 256\n",
      "SUM  : 192,946,432\n",
      "\n",
      "Config check: {'V': 151936, 'E': 4096, 'I': 12288, 'L': 36, 'H': 32, 'H_kv': 8, 'D': 128, 'TIE': False}\n",
      "\n",
      "=== Per-layer counts from built model (no weights) ===\n",
      "[layer 0]  attn=41,943,040  mlp=150,994,944  rms=8,192  qkn=256  sum=192,946,432\n",
      "[layer 1]  attn=41,943,040  mlp=150,994,944  rms=8,192  qkn=256  sum=192,946,432\n",
      "[layer 35]  attn=41,943,040  mlp=150,994,944  rms=8,192  qkn=256  sum=192,946,432\n",
      "\n",
      "Distinct values across layers:\n",
      "attn: {'41,943,040'}\n",
      "mlp : {'150,994,944'}\n",
      "rms : {'8,192'}\n",
      "qkn : {'256'}\n",
      "sum : {'192,946,432'}\n",
      "\n",
      "=== Comparison (per-layer) ===\n",
      "attn               model=41,943,040  | expect=41,943,040  -> OK\n",
      "mlp                model=150,994,944  | expect=150,994,944  -> OK\n",
      "rms                model=8,192  | expect=8,192  -> OK\n",
      "qkn                model=256  | expect=256  -> OK\n",
      "sum                model=192,946,432  | expect=192,946,432  -> OK\n",
      "\n",
      "=== Totals ===\n",
      "blocks total       model=6,946,071,552  | expect=6,946,071,552  -> OK\n",
      "embed_tokens       model=622,329,856  | expect=622,329,856  -> OK\n",
      "lm_head            model=622,329,856  | expect=622,329,856  -> OK\n",
      "GRAND TOTAL        model=8,190,731,264  | expect=8,190,731,264  -> OK\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "# ---------------------------\n",
    "# Qwen3-8B config\n",
    "# ---------------------------\n",
    "V   = 151_936     # vocab_size\n",
    "E   = 4_096       # hidden_size\n",
    "I   = 12_288      # intermediate_size (SwiGLU)\n",
    "L   = 36          # num_hidden_layers\n",
    "H   = 32          # num_attention_heads\n",
    "H_kv= 8           # num_key_value_heads (GQA)\n",
    "D   = 128         # head_dim (E == H * D)\n",
    "TIE = False       # tie_word_embeddings (Qwen3-8B untied)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Hardcoded formula counts\n",
    "# ---------------------------\n",
    "def hardcoded_per_layer_counts(V, E, I, L, H, H_kv, D, TIE):\n",
    "    # Attention (no biases): q(E->E) + k(E->Hkv*D) + v(E->Hkv*D) + o(E->E)\n",
    "    attn = (E*E) + (E*(H_kv*D)) + (E*(H_kv*D)) + (E*E)           # = (2 + 2*H_kv/H) * E^2\n",
    "    # MLP (SwiGLU, no biases): gate(E->I) + up(E->I) + down(I->E)\n",
    "    mlp  = (E*I) + (E*I) + (I*E)                                  # = 3 * E * I\n",
    "    # RMSNorms per layer: input + post (weights only)\n",
    "    rms  = 2 * E\n",
    "    # QK-Norm per layer: q_norm(D) + k_norm(D)\n",
    "    qkn  = 2 * D\n",
    "\n",
    "    per_layer = attn + mlp + rms + qkn\n",
    "    blocks    = L * per_layer\n",
    "    token_emb = V * E\n",
    "    lm_head   = 0 if TIE else (E * V)  # Qwen3-8B: untied\n",
    "\n",
    "    return {\n",
    "        \"attn\": attn,\n",
    "        \"mlp\": mlp,\n",
    "        \"rms\": rms,\n",
    "        \"qkn\": qkn,\n",
    "        \"per_layer_total\": per_layer,\n",
    "        \"blocks_total\": blocks,\n",
    "        \"token_emb\": token_emb,\n",
    "        \"lm_head\": lm_head,\n",
    "        \"grand_total\": token_emb + lm_head + blocks,\n",
    "    }\n",
    "\n",
    "hard = hardcoded_per_layer_counts(V, E, I, L, H, H_kv, D, TIE)\n",
    "\n",
    "def fmt(n: int) -> str:\n",
    "    return f\"{n:,}\"\n",
    "\n",
    "print(\"=== Hardcoded (formula) per-layer counts ===\")\n",
    "print(\"attn :\", fmt(hard[\"attn\"]))\n",
    "print(\"mlp  :\", fmt(hard[\"mlp\"]))\n",
    "print(\"rms  :\", fmt(hard[\"rms\"]))\n",
    "print(\"qkn  :\", fmt(hard[\"qkn\"]))\n",
    "print(\"SUM  :\", fmt(hard[\"per_layer_total\"]))\n",
    "print()\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2) Build model from config (no weights)\n",
    "# ----------------------------------------\n",
    "cfg = AutoConfig.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "model = AutoModelForCausalLM.from_config(cfg)   # no checkpoint download\n",
    "\n",
    "# Quick sanity that config matches what we hardcoded\n",
    "cfg_vals = dict(\n",
    "    V=cfg.vocab_size,\n",
    "    E=cfg.hidden_size,\n",
    "    I=cfg.intermediate_size,\n",
    "    L=cfg.num_hidden_layers,\n",
    "    H=cfg.num_attention_heads,\n",
    "    H_kv=getattr(cfg, \"num_key_value_heads\", H),\n",
    "    D=getattr(cfg, \"head_dim\", cfg.hidden_size // cfg.num_attention_heads),\n",
    "    TIE=getattr(cfg, \"tie_word_embeddings\", False),\n",
    ")\n",
    "print(\"Config check:\", cfg_vals)\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Count from the built model\n",
    "# ---------------------------\n",
    "def count_params(module) -> int:\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "def per_layer_model_counts(model):\n",
    "    core = model.model\n",
    "    layers = core.layers\n",
    "    L = len(layers)\n",
    "\n",
    "    per_layer = []\n",
    "    for i, block in enumerate(layers):\n",
    "        sa  = block.self_attn\n",
    "        mlp = block.mlp\n",
    "\n",
    "        # Attention projections (weights only by Qwen3 design)\n",
    "        attn_q = count_params(sa.q_proj)\n",
    "        attn_k = count_params(sa.k_proj)\n",
    "        attn_v = count_params(sa.v_proj)\n",
    "        attn_o = count_params(sa.o_proj)\n",
    "\n",
    "        # QK-Norm: present in Qwen3 (count weights if exist)\n",
    "        q_norm = count_params(sa.q_norm) if hasattr(sa, \"q_norm\") else 0\n",
    "        k_norm = count_params(sa.k_norm) if hasattr(sa, \"k_norm\") else 0\n",
    "\n",
    "        # RMSNorms\n",
    "        ln_in   = count_params(block.input_layernorm)\n",
    "        ln_post = count_params(block.post_attention_layernorm)\n",
    "\n",
    "        # MLP (SwiGLU)\n",
    "        mlp_gate = count_params(mlp.gate_proj)\n",
    "        mlp_up   = count_params(mlp.up_proj)\n",
    "        mlp_down = count_params(mlp.down_proj)\n",
    "\n",
    "        d = {\n",
    "            \"layer\": i,\n",
    "            \"attn\": attn_q + attn_k + attn_v + attn_o,\n",
    "            \"mlp\": mlp_gate + mlp_up + mlp_down,\n",
    "            \"rms\": ln_in + ln_post,\n",
    "            \"qkn\": q_norm + k_norm,\n",
    "        }\n",
    "        d[\"sum\"] = d[\"attn\"] + d[\"mlp\"] + d[\"rms\"] + d[\"qkn\"]\n",
    "        per_layer.append(d)\n",
    "\n",
    "    return per_layer\n",
    "\n",
    "pl = per_layer_model_counts(model)\n",
    "\n",
    "# Print a few layers and compare vs formula\n",
    "print(\"=== Per-layer counts from built model (no weights) ===\")\n",
    "for i in [0, 1, L-1]:\n",
    "    d = pl[i]\n",
    "    print(f\"[layer {i}]  attn={fmt(d['attn'])}  mlp={fmt(d['mlp'])}  rms={fmt(d['rms'])}  qkn={fmt(d['qkn'])}  sum={fmt(d['sum'])}\")\n",
    "\n",
    "# Check that all layers match and are identical\n",
    "all_attn = {d[\"attn\"] for d in pl}\n",
    "all_mlp  = {d[\"mlp\"]  for d in pl}\n",
    "all_rms  = {d[\"rms\"]  for d in pl}\n",
    "all_qkn  = {d[\"qkn\"]  for d in pl}\n",
    "all_sum  = {d[\"sum\"]  for d in pl}\n",
    "print(\"\\nDistinct values across layers:\")\n",
    "print(\"attn:\", {fmt(x) for x in all_attn})\n",
    "print(\"mlp :\", {fmt(x) for x in all_mlp})\n",
    "print(\"rms :\", {fmt(x) for x in all_rms})\n",
    "print(\"qkn :\", {fmt(x) for x in all_qkn})\n",
    "print(\"sum :\", {fmt(x) for x in all_sum})\n",
    "print()\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Side-by-side comparison\n",
    "# ---------------------------\n",
    "def check(label, got, expect):\n",
    "    ok = \"OK\" if got == expect else f\"DIFF ({fmt(got - expect)})\"\n",
    "    print(f\"{label:18s} model={fmt(got)}  | expect={fmt(expect)}  -> {ok}\")\n",
    "\n",
    "print(\"=== Comparison (per-layer) ===\")\n",
    "check(\"attn\", next(iter(all_attn)), hard[\"attn\"])\n",
    "check(\"mlp\",  next(iter(all_mlp)),  hard[\"mlp\"])\n",
    "check(\"rms\",  next(iter(all_rms)),  hard[\"rms\"])\n",
    "check(\"qkn\",  next(iter(all_qkn)),  hard[\"qkn\"])\n",
    "check(\"sum\",  next(iter(all_sum)),  hard[\"per_layer_total\"])\n",
    "\n",
    "# Totals (optional)\n",
    "blocks_model = sum(d[\"sum\"] for d in pl)\n",
    "token_emb_model = count_params(model.model.embed_tokens)\n",
    "lm_head_model   = count_params(model.lm_head)  # untied\n",
    "grand_model = token_emb_model + lm_head_model + blocks_model\n",
    "\n",
    "print(\"\\n=== Totals ===\")\n",
    "check(\"blocks total\", blocks_model, hard[\"blocks_total\"])\n",
    "check(\"embed_tokens\", token_emb_model, hard[\"token_emb\"])\n",
    "check(\"lm_head\",      lm_head_model,   hard[\"lm_head\"])\n",
    "check(\"GRAND TOTAL\",  grand_model,     hard[\"grand_total\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
